import logging
import argparse
import math
import os
import torch
import scipy
import nltk
import random
import numpy as np
import json
import pickle
import time
from typing import Union, List, Optional
from collections import Counter
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from peft import AutoPeftModelForCausalLM, PeftModel
from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch.nn.functional as F
from neuron_utils import enhance_neurons
from data_utils import prepare_dataset
from test_utils import test

# set logger
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser()

    # Basic parameters
    parser.add_argument("--model_to_val_path",
                        default=None,
                        type=str,
                        required=True,
                        help="Path to pretrained model or model identifier from huggingface.co/models.")
    # parser.add_argument("--data_path",
    #                     default=None,
    #                     type=str,
    #                     required=True,
    #                     help="The input data path. Should be .json file for the MLM task. ")
    parser.add_argument("--model_name",
                        default=None,
                        type=str,
                        help="Model name as output prefix to indentify each running of experiment.")
    parser.add_argument("--adversarial_method_name",
                        type=str,
                        default=None,
                        help="Dataset name as output prefix to indentify each running of experiment.")
    
    parser.add_argument("--neuron_file",
                        default=None,
                        type=str,
                        help="The file where important positions are stored.")
    parser.add_argument("--generation_dir",
                        default=None,
                        type=str,
                        required=True,
                        help="The output directory where the model predictions will be written.")
    parser.add_argument("--metric_dir",
                        default=None,
                        type=str,
                        required=True,
                        help="The output directory where the implementation metrics will be written.")
    parser.add_argument("--suffix_for_output",
                        default=None,
                        type=str,
                        help="The output suffix, which is used to form the output file name.")

    # Other parameters
    parser.add_argument("--few_data_for_debug",
                        default=False,
                        action="store_true",
                        help="When debugging, only take a small amount of data.")
    parser.add_argument("--max_new_tokens",
                        default=512,
                        type=int,
                        help="The maximum number of tokens generated by the `generate()` method.")
    parser.add_argument("--no_cuda",
                        default=False,
                        action='store_true',
                        help="Whether not to use CUDA when available.")
    parser.add_argument("--gpu_id",
                        type=str,
                        default='0',
                        help="available gpu_id id.")
    parser.add_argument('--generate_batch_size',
                        type=int,
                        default=4,
                        help="batch size of generation.")
    parser.add_argument('--eval_safety_batch_size',
                        type=int,
                        default=10,
                        help="batch size of evaluation of safety.")
    parser.add_argument('--seed',
                        type=int,
                        default=42,
                        help="random seed for initialization")
    parser.add_argument("--enhance_neuron_num",
                        type=int,
                        default=0,
                        help="The number of context-aware neurons to be erased.")
    parser.add_argument("--enhance_strength",
                        type=float,
                        default=1,
                        help="The factor by which the weights of the context neurons need to be enhanced.")

    parser.add_argument("--safety_classifier_path",
                        type=str,
                        default="/data/dans/projects/IRCAN-Safety2/SafeEdit-Safety-Classifier",
                        help="Path to the trained safety classifier model.")

    parser.add_argument("--do_random_neurons",
                        default=False,
                        action='store_true',
                        help="Set this flag if you want to randomly select neurons to operate on")
    parser.add_argument("--do_random_last_layer_neurons",
                        default=False,
                        action='store_true',
                        help="Set this flag if you want to randomly select neurons in the last layer to operate on")
    parser.add_argument("--add_chat_template",
                        default=False,
                        help="Set this flag if you want to use chat template.")
    # parser.add_argument("--instruction_prompt_type",
    #                     type=str)
    
    args = parser.parse_args()
    return args


@torch.no_grad()
def main():
    args = parse_args()
    if args.adversarial_method_name in ["Jailbroken", "SafeEditWithAdv"]:
        args.add_chat_template = False
    else:
        args.add_chat_template = True
    if args.add_chat_template:
        print("=" * 40, "add chat_template", "=" * 40)
    else:
        print("=" * 40, "NOT add chat_template", "=" * 40)
    
    # set device
    if args.no_cuda or not torch.cuda.is_available():
        device = torch.device("cpu")
        n_gpu = 0
    elif len(args.gpu_id) == 1:
        device = torch.device(f"cuda:{args.gpu_id}")
        n_gpu = 1
    else:
        # !!! to implement multi-gpus
        pass
    logger.info("device: {} n_gpu: {}, distributed training: {}".format(device, n_gpu, bool(n_gpu > 1)))

    # set random seeds
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if n_gpu > 0:
        torch.cuda.manual_seed_all(args.seed)

    # ============================== load model and tokenizer ==============================
    if "lora" in args.model_name.lower():
        print("Load LoRA model.")
        tokenizer = AutoTokenizer.from_pretrained(args.model_to_val_path)
        model = AutoPeftModelForCausalLM.from_pretrained(
            args.model_to_val_path,
            low_cpu_mem_usage=True,
            # device_map=device,
        ).to(device)
    else:
        tokenizer = AutoTokenizer.from_pretrained(args.model_to_val_path, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(
            args.model_to_val_path,
            use_cache=True,
            low_cpu_mem_usage=True,
            # device_map=device,
        ).to(device)
    # print(next(model.parameters()).device)
    # print("tokenizer.pad_token_id:", tokenizer.pad_token_id)
    # print("tokenizer.model_max_length:", tokenizer.model_max_length)

    if tokenizer.pad_token == None:
        print("add_special_tokens")
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        model.resize_token_embeddings(len(tokenizer))

    model.eval()

    # logger.info("***** CUDA.empty_cache() *****")
    # torch.cuda.empty_cache()
    print("=" * 40)
    print(f"Training has finished. Start testing the trained {args.model_name} with {args.adversarial_method_name}.")
    test(args, model, tokenizer, args.seed, device=device)


if __name__ == "__main__":
    main()